# Gradio UI Fixes - Diff Toggle and LLM Integration

## Issues Fixed

### Issue 1: Diff Button Not Working

**Problem**: Clicking [üîç Diff] button did nothing

**Root Cause**: Gradio's HTML component sanitizes JavaScript, preventing the `onclick` event handlers from executing

**Solution**: Replaced JavaScript-based toggle with native HTML `<details>` and `<summary>` elements

#### Before (JavaScript approach)
```html
<button onclick="toggleDiff('diff_1_0')">üîç Diff</button>
<div id="diff_1_0" style="display: none;">
    <pre>...diff content...</pre>
</div>

<script>
function toggleDiff(id) {
    var element = document.getElementById(id);
    element.style.display = element.style.display === 'none' ? 'block' : 'none';
}
</script>
```

**Problem**: Gradio sanitizes `onclick` and `<script>` tags for security

#### After (Native HTML approach)
```html
<details>
    <summary style='cursor: pointer;'>
        <span style='...'>üîç Diff</span>
        <code>path/to/file.java</code>
    </summary>
    <div style='...'>
        <pre>...diff content...</pre>
    </div>
</details>
```

**Benefits**:
- ‚úÖ Works in Gradio without JavaScript
- ‚úÖ Native browser support (all modern browsers)
- ‚úÖ No security concerns
- ‚úÖ Simpler code
- ‚úÖ Better accessibility

### Issue 2: LLM 404 Error

**Problem**:
```
Error calling Ollama: 404 Client Error: Not Found for url: http://localhost:11434/api/generate
[DEBUG] LLM returned 94 characters
```

**Root Cause**: Using deprecated Ollama API endpoint `/api/generate`

**Solution**: Updated to new Ollama Chat API endpoint `/api/chat` with proper message format

#### Before (Old API)
```python
url = f"{self.config.api_base}/api/generate"

payload = {
    "model": self.config.model_name,
    "prompt": full_prompt,
    "temperature": self.config.temperature,
    "stream": False
}

response = requests.post(url, json=payload)
result = response.json()
return result.get("response", "")
```

**Problem**: `/api/generate` was deprecated in newer Ollama versions

#### After (New Chat API)
```python
url = f"{self.config.api_base}/api/chat"

# Build messages array for chat API
messages = []
if system_prompt:
    messages.append({
        "role": "system",
        "content": system_prompt
    })
messages.append({
    "role": "user",
    "content": prompt
})

payload = {
    "model": self.config.model_name,
    "messages": messages,
    "stream": False,
    "options": {
        "temperature": self.config.temperature,
        "num_predict": self.config.max_tokens,
        "top_p": self.config.top_p
    }
}

response = requests.post(url, json=payload)
result = response.json()
# Extract response from message content
if "message" in result and "content" in result["message"]:
    return result["message"]["content"]
return result.get("response", "")
```

**Benefits**:
- ‚úÖ Compatible with newer Ollama versions
- ‚úÖ Proper chat message format
- ‚úÖ System prompts handled correctly
- ‚úÖ Better structured responses

## Files Modified

### 1. ragmcp/gradio_ui.py

**Changes**:
1. Replaced `<button onclick="...">` with `<details><summary>` (lines 493-503)
2. Removed JavaScript `<script>` block (was lines 529-539)

**Impact**: Diff toggles now work reliably in Gradio

### 2. ragmcp/llm_integration.py

**Changes**:
1. Updated endpoint from `/api/generate` to `/api/chat` (line 195)
2. Changed payload structure to use `messages` array (lines 198-207)
3. Updated response parsing to extract from `message.content` (lines 226-228)

**Impact**: LLM integration now works with modern Ollama versions

## How the Fixes Work

### Native HTML Details/Summary

The `<details>` element is a standard HTML5 disclosure widget:

```html
<details>               <!-- Collapsible container -->
    <summary>           <!-- Clickable header (always visible) -->
        Click me!
    </summary>
    <div>               <!-- Content (hidden by default) -->
        Hidden content
    </div>
</details>
```

**Behavior**:
- Closed by default: Summary visible, content hidden
- Click summary: Content becomes visible
- Click again: Content hides
- No JavaScript needed
- Works in Gradio HTML components

**Browser Support**: All modern browsers (Chrome, Firefox, Safari, Edge)

### Ollama Chat API Format

The Chat API uses a conversational message structure:

```json
{
  "model": "qwen2.5-coder:1.5b",
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant"
    },
    {
      "role": "user",
      "content": "What is the capital of France?"
    }
  ],
  "options": {
    "temperature": 0.7,
    "num_predict": 2048
  }
}
```

**Response Format**:
```json
{
  "model": "qwen2.5-coder:1.5b",
  "message": {
    "role": "assistant",
    "content": "The capital of France is Paris."
  },
  "created_at": "2024-01-08T...",
  "done": true
}
```

**Migration from Old API**:
- Old: `prompt` (string) ‚Üí New: `messages` (array)
- Old: `result["response"]` ‚Üí New: `result["message"]["content"]`

## Testing the Fixes

### Test 1: Diff Toggle

1. Launch UI: `launch_ui.bat`
2. Go to "Task Search" tab
3. Enter query: "Add OAuth authentication"
4. Click "üîç Find Similar Tasks"
5. Look for similar tasks with "üìÅ Changed Files"
6. **Click the file path line** (entire summary is clickable)
7. **Expected**: Diff expands smoothly below the file path
8. **Click again**: Diff collapses

**Visual Feedback**:
- Closed: `‚ñ∂ üîç Diff path/to/file.java`
- Open: `‚ñº üîç Diff path/to/file.java` with diff content below

### Test 2: LLM Integration

1. Launch UI: `launch_ui.bat`
2. Go to "RAG + LLM Analysis" tab
3. Select LLM: "ollama-qwen" or "ollama-codellama"
4. Enter query: "Fix memory leak in network buffer pool"
5. Click "üîç Search with LLM"
6. **Expected**:
   - No 404 errors
   - LLM generates analysis
   - Results appear in output

**Debug Output**:
```
[DEBUG] Selected LLM: ollama (model: qwen2.5-coder:1.5b)
[DEBUG] Prompt: [Task]\n...
[DEBUG] LLM returned 1234 characters
[DEBUG] First 200 chars: Based on the task description...
```

## Ollama Version Compatibility

### Old API (/api/generate)
- **Deprecated**: Ollama v0.1.x
- **Removed**: Ollama v0.2.0+
- **Error**: 404 Not Found

### New API (/api/chat)
- **Introduced**: Ollama v0.1.15
- **Current**: All versions v0.1.15+
- **Status**: ‚úÖ Active and supported

### Check Your Ollama Version
```bash
ollama --version
```

**If you see 404 errors**, update Ollama:
```bash
# Download latest from https://ollama.ai
# Or update via package manager
```

## Troubleshooting

### Issue: Diff still not toggling

**Symptoms**: Clicking diff area does nothing

**Possible causes**:
1. Browser cache not cleared
2. Old HTML cached by Gradio
3. Custom CSS blocking pointer events

**Solution**:
1. Hard refresh browser: `Ctrl+F5` (Windows) or `Cmd+Shift+R` (Mac)
2. Restart Gradio UI: Close and run `launch_ui.bat` again
3. Try different browser
4. Check browser console for errors (F12 ‚Üí Console tab)

### Issue: LLM still getting 404

**Symptoms**:
```
Error calling Ollama: 404 Client Error: Not Found for url: http://localhost:11434/api/chat
```

**Possible causes**:
1. Ollama version too old (pre-v0.1.15)
2. Ollama not running
3. Different port

**Solution**:
1. Check Ollama version: `ollama --version`
2. Update if needed: Download from https://ollama.ai
3. Verify Ollama is running: `ollama list`
4. Check port in config.py: `OLLAMA_URL = 'http://localhost:11434'`

### Issue: Diff expands but shows "No diff available"

**Symptoms**: Diff section opens but only shows placeholder text

**Possible causes**:
1. RAWDATA table doesn't have diff for that file
2. Task name mismatch
3. Diff column is NULL in database

**Solution**:
1. Check RAWDATA table:
   ```sql
   SELECT task_name, path, LENGTH(diff) as diff_size
   FROM vectors.rawdata
   WHERE task_name = 'SONAR-12345'
   LIMIT 10;
   ```
2. Verify data exists
3. Re-run ETL if data is missing

### Issue: LLM returns empty response

**Symptoms**:
```
[DEBUG] LLM returned 0 characters
```

**Possible causes**:
1. Model not pulled
2. System overloaded
3. Prompt too long

**Solution**:
1. Pull model: `ollama pull qwen2.5-coder:1.5b`
2. Check system resources (RAM, CPU)
3. Try smaller model: `ollama pull gemma3:1b`
4. Reduce prompt length in gradio_ui.py

## Additional Improvements

### Styling Enhancements

The `<details>` approach allows better styling:

```html
<summary style='cursor: pointer; font-weight: bold; padding: 4px; list-style: none;'>
```

**Key CSS**:
- `cursor: pointer` - Shows hand cursor on hover
- `list-style: none` - Removes default triangle marker
- `font-weight: bold` - Makes summary stand out

### Accessibility

Native `<details>` provides:
- ‚úÖ Keyboard navigation (Space/Enter to toggle)
- ‚úÖ Screen reader support
- ‚úÖ Semantic HTML
- ‚úÖ No custom ARIA attributes needed

### Performance

**Before (JavaScript)**:
- DOM queries on every click
- Event listener overhead
- Potential memory leaks

**After (Native HTML)**:
- Browser-native implementation
- Zero JavaScript overhead
- Automatic memory management

## Summary

**Problems Fixed**:
1. ‚úÖ Diff toggle not working ‚Üí Use native `<details>` element
2. ‚úÖ LLM 404 error ‚Üí Update to `/api/chat` endpoint

**Files Modified**:
1. `ragmcp/gradio_ui.py` - Diff toggle implementation
2. `ragmcp/llm_integration.py` - Ollama API endpoint

**Benefits**:
- Diff toggles work reliably without JavaScript
- LLM integration compatible with modern Ollama
- Simpler, more maintainable code
- Better browser compatibility
- Improved accessibility

**Test Commands**:
```bash
# Test Gradio UI
cd C:\Project\codeXplorer\capestone\simrgl\ragmcp
launch_ui.bat

# Verify Ollama
ollama --version
ollama list
```

**Expected Results**:
- Diffs expand/collapse on click
- LLM generates analysis without 404 errors
