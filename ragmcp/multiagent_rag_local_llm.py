#!/usr/bin/env python3
"""
–ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∏–π RAG –∑ –ª–æ–∫–∞–ª—å–Ω–∏–º–∏ LLM (CodeBERT + Qwen3 4B)
–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î MCP –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–Ω–æ–≥–æ –ø–æ—à—É–∫—É –º–æ–¥—É–ª—ñ–≤
"""

import asyncio
import json
from typing import List, Dict, Any
from dataclasses import dataclass
import torch
from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client


@dataclass
class AgentConfig:
    """–ö–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—è –∞–≥–µ–Ω—Ç–∞"""
    name: str
    model_name: str
    role: str
    temperature: float = 0.7


class LocalLLMAgent:
    """–ê–≥–µ–Ω—Ç –Ω–∞ –±–∞–∑—ñ –ª–æ–∫–∞–ª—å–Ω–æ—ó LLM"""
    
    def __init__(self, config: AgentConfig):
        self.config = config
        self.tokenizer = None
        self.model = None
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
    async def initialize(self):
        """–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ"""
        print(f"–ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è {self.config.name} ({self.config.model_name})...")
        
        if "codebert" in self.config.model_name.lower():
            # CodeBERT –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É –∫–æ–¥—É
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.model_name)
            self.model = AutoModel.from_pretrained(self.config.model_name)
        else:
            # Qwen3 –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó —Ç–µ–∫—Å—Ç—É
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.model_name,
                trust_remote_code=True
            )
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.model_name,
                torch_dtype=torch.float16,
                device_map="auto",
                trust_remote_code=True
            )
        
        self.model.to(self.device)
        print(f"{self.config.name} –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ –Ω–∞ {self.device}")
    
    async def generate(self, prompt: str, max_length: int = 512) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ"""
        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_length=max_length,
                temperature=self.config.temperature,
                do_sample=True,
                top_p=0.9
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response.replace(prompt, "").strip()
    
    async def embed(self, text: str) -> torch.Tensor:
        """–û—Ç—Ä–∏–º–∞–Ω–Ω—è embedding (–¥–ª—è CodeBERT)"""
        inputs = self.tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            # Mean pooling
            embeddings = outputs.last_hidden_state.mean(dim=1)
        
        return embeddings


class MultiAgentRAG:
    """–ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞ RAG —Å–∏—Å—Ç–µ–º–∞"""
    
    def __init__(self):
        # –ê–≥–µ–Ω—Ç–∏
        self.code_analyzer = None  # CodeBERT
        self.response_generator = None  # Qwen3
        
        # MCP –∫–ª—ñ—î–Ω—Ç –¥–ª—è semantic fingerprint
        self.mcp_session = None
        
    async def initialize(self):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –≤—Å—ñ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ñ–≤"""
        print("–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ—ó RAG —Å–∏—Å—Ç–µ–º–∏...")
        
        # 1. –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∞–≥–µ–Ω—Ç—ñ–≤
        self.code_analyzer = LocalLLMAgent(AgentConfig(
            name="Code Analyzer",
            model_name="microsoft/codebert-base",
            role="–ê–Ω–∞–ª—ñ–∑ –∫–æ–¥—É —Ç–∞ —Ç–µ—Ö–Ω—ñ—á–Ω–∏—Ö –æ–ø–∏—Å—ñ–≤",
            temperature=0.3
        ))
        await self.code_analyzer.initialize()
        
        self.response_generator = LocalLLMAgent(AgentConfig(
            name="Response Generator",
            model_name="Qwen/Qwen2.5-Coder-1.5B-Instruct",  # –∞–±–æ Qwen3 4B —è–∫—â–æ –¥–æ—Å—Ç—É–ø–Ω–æ
            role="–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥–µ–π —Ç–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π",
            temperature=0.7
        ))
        await self.response_generator.initialize()
        
        # 2. –ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ MCP —Å–µ—Ä–≤–µ—Ä–∞
        print("–ü—ñ–¥–∫–ª—é—á–µ–Ω–Ω—è –¥–æ MCP —Å–µ—Ä–≤–µ—Ä–∞...")
        server_params = StdioServerParameters(
            command="python",
            args=["semantic_fingerprint_mcp_server.py"],
            env=None
        )
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–∏–π –º–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è –∫–ª—ñ—î–Ω—Ç–∞
        self.mcp_client_context = stdio_client(server_params)
        self.read_stream, self.write_stream = await self.mcp_client_context.__aenter__()
        
        # –°—Ç–≤–æ—Ä—é—î–º–æ —Å–µ—Å—ñ—é
        self.mcp_session = ClientSession(self.read_stream, self.write_stream)
        await self.mcp_session.__aenter__()
        
        # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è —Å–µ—Å—ñ—ó
        await self.mcp_session.initialize()
        
        print("‚úì –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –¥–æ —Ä–æ–±–æ—Ç–∏")
    
    async def cleanup(self):
        """–û—á–∏—â–µ–Ω–Ω—è —Ä–µ—Å—É—Ä—Å—ñ–≤"""
        if self.mcp_session:
            await self.mcp_session.__aexit__(None, None, None)
        if hasattr(self, 'mcp_client_context'):
            await self.mcp_client_context.__aexit__(None, None, None)
    
    async def search_modules(self, task_description: str, project: str) -> List[Dict]:
        """–ü–æ—à—É–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –º–æ–¥—É–ª—ñ–≤ —á–µ—Ä–µ–∑ MCP"""
        result = await self.mcp_session.call_tool(
            "search_modules",
            arguments={
                "task_description": task_description,
                "top_k": 10,
                "project": project
            }
        )
        
        # –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É
        if result.content:
            text_content = result.content[0].text
            # –í–∏—Ç—è–≥—É—î–º–æ JSON –∑ —Ç–µ–∫—Å—Ç—É
            json_start = text_content.find('[')
            if json_start != -1:
                json_text = text_content[json_start:]
                # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –∫—ñ–Ω–µ—Ü—å JSON
                bracket_count = 0
                for i, char in enumerate(json_text):
                    if char == '[':
                        bracket_count += 1
                    elif char == ']':
                        bracket_count -= 1
                        if bracket_count == 0:
                            json_text = json_text[:i+1]
                            break
                return json.loads(json_text)
        
        return []
    
    async def analyze_code_context(self, modules: List[Dict], task_desc: str) -> str:
        """–ê–Ω–∞–ª—ñ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É –∫–æ–¥—É —á–µ—Ä–µ–∑ CodeBERT"""
        # –§–æ—Ä–º—É—î–º–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç –∑ –º–æ–¥—É–ª—ñ–≤
        context = f"Task: {task_desc}\n\nRelevant modules:\n"
        for i, mod in enumerate(modules[:5], 1):
            context += f"{i}. {mod['module']} (similarity: {mod['similarity']})\n"
        
        # CodeBERT –∞–Ω–∞–ª—ñ–∑—É—î —Ç–µ—Ö–Ω—ñ—á–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        analysis_prompt = f"""Analyze the technical context:
{context}

Provide a brief technical analysis of which modules are most relevant and why."""
        
        analysis = await self.code_analyzer.generate(analysis_prompt, max_length=256)
        return analysis
    
    async def generate_response(
        self, 
        task_description: str, 
        modules: List[Dict], 
        code_analysis: str
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ñ—ñ–Ω–∞–ª—å–Ω–æ—ó –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ —á–µ—Ä–µ–∑ Qwen3"""
        prompt = f"""<|im_start|>system
You are a helpful AI assistant for software development. You help developers find relevant code modules.
<|im_end|>
<|im_start|>user
Task: {task_description}

Relevant modules found:
{json.dumps(modules[:5], indent=2)}

Technical analysis:
{code_analysis}

Please provide:
1. Summary of the most relevant modules
2. Recommendations on where to start
3. Potential concerns or considerations
<|im_end|>
<|im_start|>assistant
"""
        
        response = await self.response_generator.generate(prompt, max_length=1024)
        return response
    
    async def process_query(self, task_description: str, project: str) -> Dict[str, Any]:
        """–û–±—Ä–æ–±–∫–∞ –∑–∞–ø–∏—Ç—É –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞ (–≥–æ–ª–æ–≤–Ω–∏–π workflow)"""
        print(f"\n{'='*60}")
        print(f"–û–±—Ä–æ–±–∫–∞ –∑–∞–ø–∏—Ç—É: {task_description[:80]}...")
        print(f"{'='*60}\n")
        
        # –ö—Ä–æ–∫ 1: –ü–æ—à—É–∫ –º–æ–¥—É–ª—ñ–≤ —á–µ—Ä–µ–∑ MCP
        print("üîç –ö—Ä–æ–∫ 1: –ü–æ—à—É–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –º–æ–¥—É–ª—ñ–≤...")
        modules = await self.search_modules(task_description, project)
        print(f"   –ó–Ω–∞–π–¥–µ–Ω–æ {len(modules)} –º–æ–¥—É–ª—ñ–≤")
        
        # –ö—Ä–æ–∫ 2: –ê–Ω–∞–ª—ñ–∑ –∫–æ–¥—É —á–µ—Ä–µ–∑ CodeBERT
        print("ü§ñ –ö—Ä–æ–∫ 2: –ê–Ω–∞–ª—ñ–∑ —Ç–µ—Ö–Ω—ñ—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É (CodeBERT)...")
        code_analysis = await self.analyze_code_context(modules, task_description)
        print(f"   –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        
        # –ö—Ä–æ–∫ 3: –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ —á–µ—Ä–µ–∑ Qwen3
        print("üí¨ –ö—Ä–æ–∫ 3: –ì–µ–Ω–µ—Ä–∞—Ü—ñ—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ–π (Qwen3)...")
        final_response = await self.generate_response(
            task_description, 
            modules, 
            code_analysis
        )
        print(f"   –í—ñ–¥–ø–æ–≤—ñ–¥—å –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–æ")
        
        return {
            "task": task_description,
            "modules": modules,
            "code_analysis": code_analysis,
            "recommendations": final_response
        }


async def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—è —Ä–æ–±–æ—Ç–∏ —Å–∏—Å—Ç–µ–º–∏"""
    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è
    rag = MultiAgentRAG()
    await rag.initialize()
    
    try:
        # –ü—Ä–∏–∫–ª–∞–¥–∏ –∑–∞–ø–∏—Ç—ñ–≤
        test_queries = [
            {
                "task": "Fix memory leak in network buffer pool",
                "project": "flink"
            },
            {
                "task": "Add support for custom SQL functions in table API",
                "project": "flink"
            },
            {
                "task": "Improve code quality analysis for JavaScript",
                "project": "sonar"
            }
        ]
        
        for query in test_queries:
            result = await rag.process_query(query["task"], query["project"])
            
            print(f"\n{'='*60}")
            print("–†–ï–ó–£–õ–¨–¢–ê–¢:")
            print(f"{'='*60}")
            print(f"\n–ó–∞–¥–∞—á–∞: {result['task']}")
            print(f"\n–¢–æ–ø-5 –º–æ–¥—É–ª—ñ–≤:")
            for i, mod in enumerate(result['modules'][:5], 1):
                print(f"  {i}. {mod['module']} (—Å—Ö–æ–∂—ñ—Å—Ç—å: {mod['similarity']})")
            
            print(f"\n–¢–µ—Ö–Ω—ñ—á–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ (CodeBERT):")
            print(f"  {result['code_analysis'][:200]}...")
            
            print(f"\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó (Qwen3):")
            print(f"  {result['recommendations'][:300]}...")
            print(f"\n{'='*60}\n")
            
            await asyncio.sleep(1)
    
    finally:
        await rag.cleanup()


if __name__ == "__main__":
    asyncio.run(main())
