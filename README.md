# CodeXplorer Research Experiments

A comprehensive collection of experiments investigating task-to-code prediction using different machine learning and information retrieval approaches.

## ðŸŽ¯ Research Goal

**Can we automatically predict which code modules/files should be modified based on a task description?**

This research explores different approaches to linking natural language task descriptions (from issue trackers like Jira) to relevant code artifacts, helping developers quickly locate relevant code for new tasks.

---

## ðŸ“Š Experiments Overview

| Experiment | Approach | Accuracy (MAP@10) | Runtime | Status | Recommendation |
|------------|----------|-------------------|---------|--------|----------------|
| **exp0** | TF-IDF | 0.5-1.5% | 4-48 hours | âŒ Failed | Historical only |
| **exp1** | Statistical Analysis | N/A (exploratory) | Fast | âœ… Complete | For insights |
| **exp3** | Dense Embeddings (BERT) | 2.3-3.5% | 20-40 min | âœ… Best | **Use this** |
| **ragmcp** | MCP Server + Local AI Agent | N/A (production tool) | Real-time | âœ… Production | **Deployable** |

### Evolution Timeline

```
exp0 (TF-IDF)
    â†“ [Failed: too slow, poor results]
exp1 (Statistical Analysis)
    â†“ [Insights: term distributions, module relationships]
exp3 (Embeddings + RAG)
    â†“ [Success: 2-3Ã— better accuracy, 100Ã— faster]
ragmcp (MCP + Local Agent)
    âœ“ [Production: MCP server, offline AI agent, PostgreSQL backend]
```

---

## ðŸ“ Experiment Details

### Experiment 0: TF-IDF Approach âŒ Not Recommended

**Location**: `exp0/`

**Approach**: Traditional TF-IDF (Term Frequency-Inverse Document Frequency) scoring to rank code modules based on term overlap with task descriptions.

**Key Scripts**:
- `taskTokenizer.py` - Tokenize task descriptions
- `TFIDF_module_token.py` - Calculate TF-IDF scores per module
- `tfidfFast.py` - Faster sklearn-based implementation

**Results**:
- âŒ **Accuracy**: 0.5-1.5% MAP@10
- â±ï¸ **Runtime**: 4-48 hours for full dataset
- ðŸ’¾ **Memory**: High (large sparse matrices)

**Why it failed**:
- Cannot understand semantics (synonyms, context)
- Too sparse (most term-module pairs are zero)
- Linear scoring insufficient for complex relationships
- Computationally inefficient

**When to read**:
- Understanding why traditional IR fails for code
- Establishing baseline for comparison
- Research transparency and methodology documentation

ðŸ“– **Full documentation**: `exp0/README.md`

---

### Experiment 1: Statistical Analysis âœ… Exploratory

**Location**: `exp1/`

**Approach**: Statistical analysis of term distributions, module hierarchies, and co-occurrence patterns.

**Key Scripts**:
- `title_term.py` - Extract terms from task titles
- `module_task.py` - Build hierarchical file/folder structure
- `term_rank.py` - Calculate term ranking metrics (HHI, composite index)
- `interlink.py` - Co-occurrence matrices for terms and files

**Outputs**:
- `TITLE_TERM` - Unique vocabulary
- `MODULE` - Hierarchical file structure
- `TERM_RANK` - Term specificity metrics
- `TERM_LINKS` - Term co-occurrence patterns
- `FILE_LINKS` - File co-modification patterns

**Insights gained**:
- Distribution of general vs. specific terms
- Module coupling through shared tasks
- Term concentration metrics (HHI)
- File modification patterns

**Use cases**:
- Understanding codebase structure
- Identifying module boundaries
- Finding coupled components
- Vocabulary analysis

**When to use**:
- Research into code structure and terminology
- Identifying refactoring opportunities
- Understanding domain vocabulary

ðŸ“– **Full documentation**: `exp1/README.md`

---

### Experiment 3: Embedding-Based RAG âœ… **RECOMMENDED**

**Location**: `exp3/`

**Approach**: Retrieval-Augmented Generation (RAG) using sentence transformers (BERT-based embeddings) and vector similarity search.

**Key Components**:
- **Embeddings**: BAAI/bge-small-en-v1.5 (and other models)
- **Vector DB**: Qdrant for similarity search
- **Aggregation**: Centroid-based file/module embeddings
- **UI**: Streamlit for interactive exploration

**Key Scripts**:
- `etl_pipeline.py` - Data processing and embedding generation
- `run_experiments.py` - Systematic evaluation
- `experiment_ui.py` - Interactive web interface
- `backup_restore_qdrant.py` - Vector DB management

**Research Questions Investigated**:

| RQ | Question | Finding |
|----|----------|---------|
| RQ1 | File vs Module granularity | Module-level better recall, file-level better precision |
| RQ2 | Title vs Description | Descriptions provide better semantic signal |
| RQ3 | Impact of comments | Comments add noise, decrease performance |
| RQ4 | Recent vs full history | Recent history reduces obsolete associations |

**Results**:
- âœ… **Accuracy**: 2.3-3.5% MAP@10 (2-3Ã— better than TF-IDF)
- âš¡ **Runtime**: 20-40 minutes (10-100Ã— faster than TF-IDF)
- ðŸŽ¯ **MRR**: 4.5-6.0% (first result quality)
- ðŸ“ˆ **Recall@10**: 3.4-4.5%

**Features**:
- Multiple embedding models support
- Configurable experiment variants
- Interactive search interface
- Comprehensive evaluation metrics
- Easy backup/restore

**When to use**:
- **Production task-to-code recommendation**
- Research on semantic code search
- Comparing embedding models
- Understanding modern RAG systems

ðŸ“– **Full documentation**: `exp3/README.md`

---

### Production Tool: MCP Server + Local AI Agent âœ… **PRODUCTION READY**

**Location**: `ragmcp/`

**Approach**: Production-ready deployment of exp3 results using Model Context Protocol (MCP) and local LLM integration.

**Key Components**:
- **MCP Server**: PostgreSQL-based semantic search server
- **Local AI Agent**: 100% offline coding assistant (CLI + Web)
- **Vector DB**: PostgreSQL + pgvector (27 modules, 12,532 files, 9,799 tasks)
- **LLM**: Ollama with qwen2.5-coder for local reasoning
- **Backup/Restore**: Full data backup and recovery

**Key Scripts**:
- `mcp_server_postgres.py` - MCP server for Claude Desktop / VS Code
- `local_agent.py` - CLI offline agent
- `local_agent_web.py` - Web interface (Gradio)
- `backup_data_from_postgree.bat/sh` - PostgreSQL backup
- `test_mcp_server.py` - Server testing

**Features**:
- âœ… **100% Offline**: No cloud services, no API keys needed
- âœ… **Privacy-First**: All data stays on your machine
- âœ… **Free Forever**: No subscriptions, no costs
- âœ… **Multi-Interface**: CLI, Web, Claude Desktop, VS Code
- âœ… **Historical Context**: 9,799 task embeddings for similarity search
- âœ… **MCP Protocol**: Standard integration with AI tools

**MCP Tools Available**:
1. `search_modules` - Find relevant code modules (top-K)
2. `search_files` - Find relevant files (top-K)
3. `search_similar_tasks` - Find historical similar tasks
4. `get_collections_info` - Collection statistics

**Interfaces**:

**CLI Mode:**
```bash
cd ragmcp/
./start_local_agent.bat    # Windows
./start_local_agent.sh     # Linux/Mac
```

**Web Interface:**
```bash
cd ragmcp/
./start_local_agent_web.bat    # Windows
# Open: http://127.0.0.1:7861
```

**Claude Desktop Integration:**
- Configure MCP server in Claude Desktop settings
- Use semantic search directly in Claude
- See `ragmcp/MCP_SETUP_GUIDE.md` for setup

**VS Code Integration:**
- Use with Cline or Continue extensions
- Terminal integration
- Task configuration
- See `ragmcp/LOCAL_AGENT_GUIDE.md` for details

**Example Query:**
```
> Fix authentication bug in login module

Agent provides:
- Relevant modules: server, sonar-server
- Relevant files: SimpleSessionsContainer.js, cookies.ts
- Similar tasks: SONAR-8493, SONAR-11066, SONAR-16181
- AI recommendations based on historical context
```

**Performance**:
- âš¡ **Search**: <100ms per query
- ðŸ¤– **LLM**: 5-10 seconds (after first query)
- ðŸ’¾ **Storage**: 112 MB (PostgreSQL)
- ðŸ”„ **Backup**: Full backup in 30 seconds

**Comparison with Cloud Solutions**:

| Feature | Local Agent | Claude Desktop | GitHub Copilot |
|---------|------------|----------------|----------------|
| Offline | âœ… Yes | âŒ No | âŒ No |
| Cost | âœ… Free | âŒ $20/month | âŒ $10/month |
| Privacy | âœ… 100% local | âŒ Cloud | âŒ Cloud |
| Codebase Search | âœ… Yes (MCP) | âœ… Yes (MCP) | âŒ Limited |
| Historical Tasks | âœ… Yes (9,799) | âŒ No | âŒ No |

**When to use**:
- **Production deployment** of task-to-code recommendations
- **Privacy-sensitive** projects (no data leaves your machine)
- **Offline development** environments
- **Integration** with Claude Desktop or VS Code
- **Free alternative** to cloud AI assistants

ðŸ“– **Full documentation**: `ragmcp/README.md`
ðŸ“– **Setup guides**: `ragmcp/MCP_SETUP_GUIDE.md`, `ragmcp/LOCAL_AGENT_GUIDE.md`

---

## ðŸš€ Quick Start Guide

### For Production Use (Recommended - Offline AI Agent)

If you want a deployable, offline AI coding assistant:

```bash
cd ragmcp/

# 1. Ensure PostgreSQL is running
podman start semantic_vectors_db

# 2. Ensure Ollama is running
ollama serve

# 3. Start the local agent (CLI)
./start_local_agent.bat    # Windows
./start_local_agent.sh     # Linux/Mac

# Or start web interface
./start_local_agent_web.bat    # Windows
# Open: http://127.0.0.1:7861
```

**What you get:**
- 100% offline AI coding assistant
- Semantic search across 12,532 files and 9,799 historical tasks
- LLM-powered recommendations
- No cloud services, no costs
- Privacy-first (all data stays local)

See `ragmcp/README.md` for complete documentation.

### For Research & Experiments

If you want to run research experiments or compare embedding models:

```bash
cd exp3/

# 1. Install dependencies
pip install -r requirements.txt

# 2. Start Qdrant database
docker-compose up -d

# 3. Run the full pipeline (automated)
./start.sh    # Linux/Mac
start.bat     # Windows

# 4. Access the UI at http://localhost:8501
```

**Alternative**: Just view results without running experiments:
```bash
./quick_start.sh    # Linux/Mac
quick_start.bat     # Windows
```

### For Research & Analysis

If you want to understand codebase structure:

```bash
cd exp1/

# Update database path in each script
# Then run in order:
python title_term.py
python module_task.py
python term_rank.py
python interlink.py
```

### For Historical Comparison

If you want to understand why TF-IDF failed:

```bash
cd exp0/

# Read the warnings first!
# Then run the fast version:
python tfidfFast.py
```

---

## ðŸ“‹ Prerequisites

### Required for All Experiments

1. **SQLite Database** with tables:
   - `RAWDATA` - Git commit data
   - `TASK` - Jira task data

   Create using the data gathering tool in `../../data_gathering/refactor/`

2. **Python 3.8+** with pip

### Experiment-Specific

**exp0**:
- pandas, numpy, nltk, scikit-learn, tqdm

**exp1**:
- tqdm only

**exp3**:
- pandas, numpy, sentence-transformers, qdrant-client, streamlit, tqdm
- Docker/Podman for Qdrant database

---

## ðŸ“Š Performance Comparison

### Accuracy Metrics

| Metric | exp0 (TF-IDF) | exp3 (Embeddings) | Improvement |
|--------|---------------|-------------------|-------------|
| MAP@10 | 0.5-1.5% | 2.3-3.5% | **2-3Ã— better** |
| MRR | 0.8-2.0% | 4.5-6.0% | **2.5-3Ã— better** |
| P@10 | 0.3-1.2% | 1.2-1.8% | **3-4Ã— better** |
| R@10 | 1.0-2.5% | 3.4-4.5% | **2-3Ã— better** |

### Runtime Performance

| Task | exp0 | exp3 | Speed-up |
|------|------|------|----------|
| ETL Pipeline | 4-48 hours | 20-30 min | **10-100Ã— faster** |
| Evaluation | N/A | 10-15 min | - |
| Query (single) | Slow | <100ms | **100-1000Ã— faster** |

### Resource Usage

| Resource | exp0 | exp1 | exp3 |
|----------|------|------|------|
| Memory | High (sparse matrices) | Low | Medium (embeddings) |
| Disk | Medium | Low | High (vectors) |
| CPU | Very high | Low | Medium |
| GPU | No | No | Optional (speeds up) |

---

## ðŸ”¬ Research Questions & Findings

### RQ1: What granularity works best?

**Experiment**: exp3, File vs Module targets

**Finding**:
- **Module-level**: Better recall (finds more relevant code)
- **File-level**: Better precision (fewer false positives)
- **Recommendation**: Use module-level for exploration, file-level for precise changes

### RQ2: How much task information is needed?

**Experiment**: exp3, Title vs Description vs Comments

**Finding**:
- **Title**: Concise but limited information
- **Description**: Best balance of semantic richness and signal-to-noise
- **Comments**: Too noisy, decreases performance
- **Recommendation**: Use Title + Description

### RQ3: Does semantic understanding matter?

**Experiment**: exp0 (term matching) vs exp3 (embeddings)

**Finding**:
- TF-IDF (term overlap): 0.5-1.5% MAP@10
- Embeddings (semantic): 2.3-3.5% MAP@10
- **Answer**: Yes, 2-3Ã— improvement with semantic understanding

### RQ4: Is historical context important?

**Experiment**: exp3, Recent vs Full history

**Finding**:
- Recent history (1000 tasks): Better for evolving codebases
- Full history: More data but includes obsolete associations
- **Recommendation**: Use recent history for active projects

---

## ðŸ“š Research Methodology

### Data Collection
1. **Git Repository**: Extract all commits with file changes
2. **Issue Tracker**: Fetch task descriptions from Jira
3. **Linking**: Match commit messages to task IDs

### Evaluation Protocol
1. **Split**:
   - Recent: Last 200 tasks for testing
   - ModN: Uniform sampling across history
2. **Training**: Build embeddings/TF-IDF from remaining tasks
3. **Querying**: For each test task, retrieve top-K code artifacts
4. **Metrics**: Calculate MAP, MRR, P@K, R@K against ground truth

### Ground Truth
- Files touched by commits associated with each task
- Assumption: Developers knew which files to modify

---

## ðŸ—‚ï¸ Project Structure

```
simrgl/
â”œâ”€â”€ README.md                    # This file
â”œâ”€â”€ exp0/                        # TF-IDF experiment (historical)
â”‚   â”œâ”€â”€ README.md               # Detailed exp0 documentation
â”‚   â”œâ”€â”€ QUICKSTART.md           # Quick start guide
â”‚   â”œâ”€â”€ config.py               # Configuration
â”‚   â”œâ”€â”€ taskTokenizer.py        # Tokenization
â”‚   â”œâ”€â”€ TFIDF_module_token.py   # Module-level TF-IDF
â”‚   â”œâ”€â”€ tfidfFast.py            # Fast sklearn TF-IDF
â”‚   â””â”€â”€ chainTfidfFast.py       # Word groups TF-IDF
â”œâ”€â”€ exp1/                        # Statistical analysis
â”‚   â”œâ”€â”€ README.md               # Detailed exp1 documentation
â”‚   â”œâ”€â”€ config.py               # Configuration
â”‚   â”œâ”€â”€ title_term.py           # Term extraction
â”‚   â”œâ”€â”€ module_task.py          # Module hierarchy
â”‚   â”œâ”€â”€ term_rank.py            # Term metrics
â”‚   â””â”€â”€ interlink.py            # Co-occurrence analysis
â”œâ”€â”€ exp3/                        # Embedding-based RAG (research)
â”‚   â”œâ”€â”€ README.md               # Comprehensive exp3 documentation
â”‚   â”œâ”€â”€ config.py               # Configuration
â”‚   â”œâ”€â”€ etl_pipeline.py         # Data processing
â”‚   â”œâ”€â”€ run_experiments.py      # Evaluation
â”‚   â”œâ”€â”€ experiment_ui.py        # Streamlit UI
â”‚   â”œâ”€â”€ utils.py                # Helper functions
â”‚   â”œâ”€â”€ docker-compose.yml      # Qdrant setup
â”‚   â”œâ”€â”€ start.sh/bat            # Automated pipeline
â”‚   â””â”€â”€ quick_start.sh/bat      # UI launcher
â””â”€â”€ ragmcp/                      # Production MCP server + Local AI agent
    â”œâ”€â”€ README.md               # Comprehensive documentation
    â”œâ”€â”€ README_UA.md            # Ukrainian version
    â”œâ”€â”€ mcp_server_postgres.py  # PostgreSQL MCP server
    â”œâ”€â”€ local_agent.py          # CLI offline agent
    â”œâ”€â”€ local_agent_web.py      # Web interface (Gradio)
    â”œâ”€â”€ gradio_ui.py            # Original Gradio UI
    â”œâ”€â”€ llm_integration.py      # LLM integration
    â”œâ”€â”€ backup_data_from_postgree.bat/sh  # Backup scripts
    â”œâ”€â”€ restore_data_to_postgree.bat/sh   # Restore scripts
    â”œâ”€â”€ start_local_agent.bat/sh          # CLI launchers
    â”œâ”€â”€ start_local_agent_web.bat/sh      # Web launchers
    â”œâ”€â”€ MCP_SETUP_GUIDE.md      # MCP integration guide
    â”œâ”€â”€ LOCAL_AGENT_GUIDE.md    # Local agent guide
    â””â”€â”€ READY_TO_USE.md         # Quick start summary
```

---

## ðŸ“– Getting Started Workflow

### Step 1: Gather Data (One-time)
```bash
cd ../../data_gathering/refactor/

# Configure your settings
vim config.py

# Run data gathering
python main.py
```

This creates a SQLite database with RAWDATA and TASK tables.

### Step 2: Run Analysis (Optional)
```bash
cd ../../capestone/claude11/simrgl/exp1/

# Generate insights about your codebase
python title_term.py
python module_task.py
python term_rank.py
python interlink.py
```

### Step 3: Build Prediction System
```bash
cd ../exp3/

# Run the full pipeline
./start.sh    # Linux/Mac
start.bat     # Windows
```

### Step 4: Use the System
- Open browser to `http://localhost:8501`
- Enter task descriptions
- Get ranked code file/module recommendations

---

## ðŸ”— Related Projects

### Production Tool: MCP Server + Local AI Agent
**Location**: `ragmcp/`

Production deployment of semantic search:
- MCP server for Claude Desktop / VS Code
- Local offline AI agent (CLI + Web)
- PostgreSQL + pgvector backend
- 100% offline, free, privacy-first
- Backup/restore capabilities
- See `ragmcp/README.md` for details

### Data Gathering Tool
**Location**: `../../data_gathering/refactor/`

Creates the database used by all experiments:
- Extracts Git commits
- Fetches Jira task details
- Links commits to tasks

### Python Scripts
**Location**: `../../python/`

Original experimental scripts (before refactoring):
- Legacy code for historical reference
- Many scripts migrated to exp0, exp1, exp3

---

## ðŸ“Š Publications & References

### Related Work

**Code Search**:
- Lv, F., et al. (2015). "CodeHow: Effective code search based on API understanding and extended Boolean model"
- Ye, X., et al. (2016). "Learning to rank relevant files for bug reports"

**Embeddings for Code**:
- Feng, Z., et al. (2020). "CodeBERT: A Pre-Trained Model for Programming and Natural Languages"
- Husain, H., et al. (2019). "CodeSearchNet Challenge"

**TF-IDF Limitations**:
- Manning, C. D., et al. (2008). "Introduction to Information Retrieval"
- Allamanis, M., et al. (2018). "A Survey of Machine Learning for Big Code"

---

## ðŸ¤ Contributing

This research is part of an academic project. Improvements welcome:

1. **exp3**: Enhancements to the embedding approach
2. **Documentation**: Clarifications and examples
3. **New experiments**: Novel approaches to task-to-code linking

Please do NOT spend time optimizing exp0 - it's a fundamental limitation of the approach.

---

## ðŸ“„ License

This is academic research code. Use for educational and research purposes.

---

## ðŸŽ“ Conclusion

### What We Learned

1. **Semantic understanding is crucial**: Traditional term matching (TF-IDF) is insufficient
2. **Embeddings work better**: 2-3Ã— improvement over TF-IDF
3. **Speed matters**: Fast iteration enables better research
4. **Context helps**: Recent history better than full history
5. **Granularity trades**: File vs module has precision/recall trade-off

### Recommended Path

**For Production Use:**
1. âœ… **Use ragmcp** (MCP server + local AI agent) - Deployable, offline, free
2. ðŸ“– **Read ragmcp/README.md** for setup instructions

**For Research:**
1. âœ… **Use exp3** (embedding-based RAG) - Best accuracy, experiments
2. ðŸ“Š **Reference exp1** (for codebase insights) - Statistical analysis
3. âŒ **Avoid exp0** (TF-IDF too slow and inaccurate) - Historical only

### Future Directions

- Fine-tuned code-specific language models (CodeBERT, GraphCodeBERT)
- Graph-based approaches (code structure graphs)
- Multi-modal learning (code + commit messages + documentation)
- Active learning for low-resource projects
- Transfer learning across projects

---

## ðŸ“§ Contact & Support

For questions about:
- **ragmcp (Production)**: See `ragmcp/README.md` for deployment guide
- **exp3 (Research)**: See `exp3/README.md` for comprehensive guide
- **exp1 (Analysis)**: See `exp1/README.md` for usage instructions
- **exp0 (Historical)**: See `exp0/README.md` for detailed documentation
- **Data gathering**: See `../../data_gathering/refactor/README.md`

---

**Built with**: Python â€¢ SQLite â€¢ Sentence Transformers â€¢ Qdrant â€¢ Streamlit

**Research conducted**: 2024-2025
