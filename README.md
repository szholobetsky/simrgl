# CodeXplorer Research Experiments

A comprehensive collection of experiments investigating task-to-code prediction using different machine learning and information retrieval approaches.

## üéØ Research Goal

**Can we automatically predict which code modules/files should be modified based on a task description?**

This research explores different approaches to linking natural language task descriptions (from issue trackers like Jira) to relevant code artifacts, helping developers quickly locate relevant code for new tasks.

---

## üìä Experiments Overview

| Experiment | Approach | Accuracy (MAP@10) | Runtime | Status | Recommendation |
|------------|----------|-------------------|---------|--------|----------------|
| **exp0** | TF-IDF | 0.5-1.5% | 4-48 hours | ‚ùå Failed | Historical only |
| **exp1** | Statistical Analysis | N/A (exploratory) | Fast | ‚úÖ Complete | For insights |
| **exp3** | Dense Embeddings (BERT) | 2.3-3.5% | 20-40 min | ‚úÖ Best | **Use this** |

### Evolution Timeline

```
exp0 (TF-IDF)
    ‚Üì [Failed: too slow, poor results]
exp1 (Statistical Analysis)
    ‚Üì [Insights: term distributions, module relationships]
exp3 (Embeddings + RAG)
    ‚úì [Success: 2-3√ó better accuracy, 100√ó faster]
```

---

## üìÅ Experiment Details

### Experiment 0: TF-IDF Approach ‚ùå Not Recommended

**Location**: `exp0/`

**Approach**: Traditional TF-IDF (Term Frequency-Inverse Document Frequency) scoring to rank code modules based on term overlap with task descriptions.

**Key Scripts**:
- `taskTokenizer.py` - Tokenize task descriptions
- `TFIDF_module_token.py` - Calculate TF-IDF scores per module
- `tfidfFast.py` - Faster sklearn-based implementation

**Results**:
- ‚ùå **Accuracy**: 0.5-1.5% MAP@10
- ‚è±Ô∏è **Runtime**: 4-48 hours for full dataset
- üíæ **Memory**: High (large sparse matrices)

**Why it failed**:
- Cannot understand semantics (synonyms, context)
- Too sparse (most term-module pairs are zero)
- Linear scoring insufficient for complex relationships
- Computationally inefficient

**When to read**:
- Understanding why traditional IR fails for code
- Establishing baseline for comparison
- Research transparency and methodology documentation

üìñ **Full documentation**: `exp0/README.md`

---

### Experiment 1: Statistical Analysis ‚úÖ Exploratory

**Location**: `exp1/`

**Approach**: Statistical analysis of term distributions, module hierarchies, and co-occurrence patterns.

**Key Scripts**:
- `title_term.py` - Extract terms from task titles
- `module_task.py` - Build hierarchical file/folder structure
- `term_rank.py` - Calculate term ranking metrics (HHI, composite index)
- `interlink.py` - Co-occurrence matrices for terms and files

**Outputs**:
- `TITLE_TERM` - Unique vocabulary
- `MODULE` - Hierarchical file structure
- `TERM_RANK` - Term specificity metrics
- `TERM_LINKS` - Term co-occurrence patterns
- `FILE_LINKS` - File co-modification patterns

**Insights gained**:
- Distribution of general vs. specific terms
- Module coupling through shared tasks
- Term concentration metrics (HHI)
- File modification patterns

**Use cases**:
- Understanding codebase structure
- Identifying module boundaries
- Finding coupled components
- Vocabulary analysis

**When to use**:
- Research into code structure and terminology
- Identifying refactoring opportunities
- Understanding domain vocabulary

üìñ **Full documentation**: `exp1/README.md`

---

### Experiment 3: Embedding-Based RAG ‚úÖ **RECOMMENDED**

**Location**: `exp3/`

**Approach**: Retrieval-Augmented Generation (RAG) using sentence transformers (BERT-based embeddings) and vector similarity search.

**Key Components**:
- **Embeddings**: BAAI/bge-small-en-v1.5 (and other models)
- **Vector DB**: Qdrant for similarity search
- **Aggregation**: Centroid-based file/module embeddings
- **UI**: Streamlit for interactive exploration

**Key Scripts**:
- `etl_pipeline.py` - Data processing and embedding generation
- `run_experiments.py` - Systematic evaluation
- `experiment_ui.py` - Interactive web interface
- `backup_restore_qdrant.py` - Vector DB management

**Research Questions Investigated**:

| RQ | Question | Finding |
|----|----------|---------|
| RQ1 | File vs Module granularity | Module-level better recall, file-level better precision |
| RQ2 | Title vs Description | Descriptions provide better semantic signal |
| RQ3 | Impact of comments | Comments add noise, decrease performance |
| RQ4 | Recent vs full history | Recent history reduces obsolete associations |

**Results**:
- ‚úÖ **Accuracy**: 2.3-3.5% MAP@10 (2-3√ó better than TF-IDF)
- ‚ö° **Runtime**: 20-40 minutes (10-100√ó faster than TF-IDF)
- üéØ **MRR**: 4.5-6.0% (first result quality)
- üìà **Recall@10**: 3.4-4.5%

**Features**:
- Multiple embedding models support
- Configurable experiment variants
- Interactive search interface
- Comprehensive evaluation metrics
- Easy backup/restore

**When to use**:
- **Production task-to-code recommendation**
- Research on semantic code search
- Comparing embedding models
- Understanding modern RAG systems

üìñ **Full documentation**: `exp3/README.md`

---

## üöÄ Quick Start Guide

### For Practical Use (Recommended)

If you want to actually predict code modules for tasks:

```bash
cd exp3/

# 1. Install dependencies
pip install -r requirements.txt

# 2. Start Qdrant database
docker-compose up -d

# 3. Run the full pipeline (automated)
./start.sh    # Linux/Mac
start.bat     # Windows

# 4. Access the UI at http://localhost:8501
```

**Alternative**: Just view results without running experiments:
```bash
./quick_start.sh    # Linux/Mac
quick_start.bat     # Windows
```

### For Research & Analysis

If you want to understand codebase structure:

```bash
cd exp1/

# Update database path in each script
# Then run in order:
python title_term.py
python module_task.py
python term_rank.py
python interlink.py
```

### For Historical Comparison

If you want to understand why TF-IDF failed:

```bash
cd exp0/

# Read the warnings first!
# Then run the fast version:
python tfidfFast.py
```

---

## üìã Prerequisites

### Required for All Experiments

1. **SQLite Database** with tables:
   - `RAWDATA` - Git commit data
   - `TASK` - Jira task data

   Create using the data gathering tool in `../../data_gathering/refactor/`

2. **Python 3.8+** with pip

### Experiment-Specific

**exp0**:
- pandas, numpy, nltk, scikit-learn, tqdm

**exp1**:
- tqdm only

**exp3**:
- pandas, numpy, sentence-transformers, qdrant-client, streamlit, tqdm
- Docker/Podman for Qdrant database

---

## üìä Performance Comparison

### Accuracy Metrics

| Metric | exp0 (TF-IDF) | exp3 (Embeddings) | Improvement |
|--------|---------------|-------------------|-------------|
| MAP@10 | 0.5-1.5% | 2.3-3.5% | **2-3√ó better** |
| MRR | 0.8-2.0% | 4.5-6.0% | **2.5-3√ó better** |
| P@10 | 0.3-1.2% | 1.2-1.8% | **3-4√ó better** |
| R@10 | 1.0-2.5% | 3.4-4.5% | **2-3√ó better** |

### Runtime Performance

| Task | exp0 | exp3 | Speed-up |
|------|------|------|----------|
| ETL Pipeline | 4-48 hours | 20-30 min | **10-100√ó faster** |
| Evaluation | N/A | 10-15 min | - |
| Query (single) | Slow | <100ms | **100-1000√ó faster** |

### Resource Usage

| Resource | exp0 | exp1 | exp3 |
|----------|------|------|------|
| Memory | High (sparse matrices) | Low | Medium (embeddings) |
| Disk | Medium | Low | High (vectors) |
| CPU | Very high | Low | Medium |
| GPU | No | No | Optional (speeds up) |

---

## üî¨ Research Questions & Findings

### RQ1: What granularity works best?

**Experiment**: exp3, File vs Module targets

**Finding**:
- **Module-level**: Better recall (finds more relevant code)
- **File-level**: Better precision (fewer false positives)
- **Recommendation**: Use module-level for exploration, file-level for precise changes

### RQ2: How much task information is needed?

**Experiment**: exp3, Title vs Description vs Comments

**Finding**:
- **Title**: Concise but limited information
- **Description**: Best balance of semantic richness and signal-to-noise
- **Comments**: Too noisy, decreases performance
- **Recommendation**: Use Title + Description

### RQ3: Does semantic understanding matter?

**Experiment**: exp0 (term matching) vs exp3 (embeddings)

**Finding**:
- TF-IDF (term overlap): 0.5-1.5% MAP@10
- Embeddings (semantic): 2.3-3.5% MAP@10
- **Answer**: Yes, 2-3√ó improvement with semantic understanding

### RQ4: Is historical context important?

**Experiment**: exp3, Recent vs Full history

**Finding**:
- Recent history (1000 tasks): Better for evolving codebases
- Full history: More data but includes obsolete associations
- **Recommendation**: Use recent history for active projects

---

## üìö Research Methodology

### Data Collection
1. **Git Repository**: Extract all commits with file changes
2. **Issue Tracker**: Fetch task descriptions from Jira
3. **Linking**: Match commit messages to task IDs

### Evaluation Protocol
1. **Split**:
   - Recent: Last 200 tasks for testing
   - ModN: Uniform sampling across history
2. **Training**: Build embeddings/TF-IDF from remaining tasks
3. **Querying**: For each test task, retrieve top-K code artifacts
4. **Metrics**: Calculate MAP, MRR, P@K, R@K against ground truth

### Ground Truth
- Files touched by commits associated with each task
- Assumption: Developers knew which files to modify

---

## üóÇÔ∏è Project Structure

```
simrgl/
‚îú‚îÄ‚îÄ README.md                    # This file
‚îú‚îÄ‚îÄ exp0/                        # TF-IDF experiment (historical)
‚îÇ   ‚îú‚îÄ‚îÄ README.md               # Detailed exp0 documentation
‚îÇ   ‚îú‚îÄ‚îÄ QUICKSTART.md           # Quick start guide
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ taskTokenizer.py        # Tokenization
‚îÇ   ‚îú‚îÄ‚îÄ TFIDF_module_token.py   # Module-level TF-IDF
‚îÇ   ‚îú‚îÄ‚îÄ tfidfFast.py            # Fast sklearn TF-IDF
‚îÇ   ‚îî‚îÄ‚îÄ chainTfidfFast.py       # Word groups TF-IDF
‚îú‚îÄ‚îÄ exp1/                        # Statistical analysis
‚îÇ   ‚îú‚îÄ‚îÄ README.md               # Detailed exp1 documentation
‚îÇ   ‚îú‚îÄ‚îÄ config.py               # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ title_term.py           # Term extraction
‚îÇ   ‚îú‚îÄ‚îÄ module_task.py          # Module hierarchy
‚îÇ   ‚îú‚îÄ‚îÄ term_rank.py            # Term metrics
‚îÇ   ‚îî‚îÄ‚îÄ interlink.py            # Co-occurrence analysis
‚îî‚îÄ‚îÄ exp3/                        # Embedding-based RAG (recommended)
    ‚îú‚îÄ‚îÄ README.md               # Comprehensive exp3 documentation
    ‚îú‚îÄ‚îÄ config.py               # Configuration
    ‚îú‚îÄ‚îÄ etl_pipeline.py         # Data processing
    ‚îú‚îÄ‚îÄ run_experiments.py      # Evaluation
    ‚îú‚îÄ‚îÄ experiment_ui.py        # Streamlit UI
    ‚îú‚îÄ‚îÄ utils.py                # Helper functions
    ‚îú‚îÄ‚îÄ docker-compose.yml      # Qdrant setup
    ‚îú‚îÄ‚îÄ start.sh/bat            # Automated pipeline
    ‚îî‚îÄ‚îÄ quick_start.sh/bat      # UI launcher
```

---

## üìñ Getting Started Workflow

### Step 1: Gather Data (One-time)
```bash
cd ../../data_gathering/refactor/

# Configure your settings
vim config.py

# Run data gathering
python main.py
```

This creates a SQLite database with RAWDATA and TASK tables.

### Step 2: Run Analysis (Optional)
```bash
cd ../../capestone/claude11/simrgl/exp1/

# Generate insights about your codebase
python title_term.py
python module_task.py
python term_rank.py
python interlink.py
```

### Step 3: Build Prediction System
```bash
cd ../exp3/

# Run the full pipeline
./start.sh    # Linux/Mac
start.bat     # Windows
```

### Step 4: Use the System
- Open browser to `http://localhost:8501`
- Enter task descriptions
- Get ranked code file/module recommendations

---

## üîó Related Projects

### Data Gathering Tool
**Location**: `../../data_gathering/refactor/`

Creates the database used by all experiments:
- Extracts Git commits
- Fetches Jira task details
- Links commits to tasks

### Python Scripts
**Location**: `../../python/`

Original experimental scripts (before refactoring):
- Legacy code for historical reference
- Many scripts migrated to exp0, exp1, exp3

---

## üìä Publications & References

### Related Work

**Code Search**:
- Lv, F., et al. (2015). "CodeHow: Effective code search based on API understanding and extended Boolean model"
- Ye, X., et al. (2016). "Learning to rank relevant files for bug reports"

**Embeddings for Code**:
- Feng, Z., et al. (2020). "CodeBERT: A Pre-Trained Model for Programming and Natural Languages"
- Husain, H., et al. (2019). "CodeSearchNet Challenge"

**TF-IDF Limitations**:
- Manning, C. D., et al. (2008). "Introduction to Information Retrieval"
- Allamanis, M., et al. (2018). "A Survey of Machine Learning for Big Code"

---

## ü§ù Contributing

This research is part of an academic project. Improvements welcome:

1. **exp3**: Enhancements to the embedding approach
2. **Documentation**: Clarifications and examples
3. **New experiments**: Novel approaches to task-to-code linking

Please do NOT spend time optimizing exp0 - it's a fundamental limitation of the approach.

---

## üìÑ License

This is academic research code. Use for educational and research purposes.

---

## üéì Conclusion

### What We Learned

1. **Semantic understanding is crucial**: Traditional term matching (TF-IDF) is insufficient
2. **Embeddings work better**: 2-3√ó improvement over TF-IDF
3. **Speed matters**: Fast iteration enables better research
4. **Context helps**: Recent history better than full history
5. **Granularity trades**: File vs module has precision/recall trade-off

### Recommended Path

For task-to-code prediction:
1. ‚úÖ **Use exp3** (embedding-based RAG)
2. üìä **Reference exp1** (for codebase insights)
3. ‚ùå **Avoid exp0** (TF-IDF too slow and inaccurate)

### Future Directions

- Fine-tuned code-specific language models (CodeBERT, GraphCodeBERT)
- Graph-based approaches (code structure graphs)
- Multi-modal learning (code + commit messages + documentation)
- Active learning for low-resource projects
- Transfer learning across projects

---

## üìß Contact & Support

For questions about:
- **exp0**: See `exp0/README.md` for detailed documentation
- **exp1**: See `exp1/README.md` for usage instructions
- **exp3**: See `exp3/README.md` for comprehensive guide
- **Data gathering**: See `../../data_gathering/refactor/README.md`

---

**Built with**: Python ‚Ä¢ SQLite ‚Ä¢ Sentence Transformers ‚Ä¢ Qdrant ‚Ä¢ Streamlit

**Research conducted**: 2024-2025
